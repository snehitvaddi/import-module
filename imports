import os
import json
import pandas as pd
from collections import defaultdict

def process_input_folder(input_folder, output_folder, threshold=70, min_threshold=30):
    """
    Process all JSON files in the input folder and save rebalanced clusters to the output folder.
    
    Args:
        input_folder: Path to folder containing JSON files
        output_folder: Path to save output cluster files
        threshold: Threshold for determining small/medium/large clusters (default: 70)
        min_threshold: Minimum threshold for cluster size (default: 30)
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    
    dfs = []
    
    print(f"Reading JSON files from: {input_folder}")
    
    file_count = 0
    for filename in os.listdir(input_folder):
        if filename.endswith('.json'):
            file_path = os.path.join(input_folder, filename)
            file_count += 1
            
            try:
                df = pd.read_json(path_or_buf=file_path, orient='index')
                df['file_name'] = filename[:-5]
                dfs.append(df)
            except Exception as e:
                print(f"Error processing {filename}: {e}")
    
    print(f"Total files processed: {file_count}")
    
    if not dfs:
        print("No valid JSON files found in the input folder.")
        return
    
    final_df = pd.concat(dfs, ignore_index=True)
    print(f"Total rows after concatenation: {len(final_df)}")
    
    filtered_df = final_df[final_df["final_sim_score"] > 0.92]
    print(f"Rows after filtering for final_sim_score > 0.92: {len(filtered_df)}")
    
    # 1. Initially check the number of pairs regardless of columns
    total_pairs = len(filtered_df) // 2
    print(f"Total number of pairs: {total_pairs}")
    
    # If less than threshold pairs (140 articles), do nothing - just save as is
    if total_pairs <= threshold:
        print(f"Total pairs ({total_pairs}) is less than threshold ({threshold}). Saving as is.")
        save_cluster(filtered_df, output_folder, 1)
        return
    
    # Continue with rebalancing
    rebalance_clusters_and_save(filtered_df, output_folder, threshold, min_threshold)
    
    # Generate final report
    saved_clusters_counts = {}
    for filename in os.listdir(output_folder):
        if filename.endswith(".json"):
            file_path = os.path.join(output_folder, filename)
            df_saved = pd.read_json(file_path, orient='index')
            saved_clusters_counts[filename] = len(df_saved) // 2
    
    print(f"Total clusters created: {len(saved_clusters_counts)}")
    for filename, pair_count in sorted(saved_clusters_counts.items()):
        print(f"{filename}: {pair_count} pairs")

def rebalance_clusters_and_save(df, output_folder, threshold=70, min_threshold=30):
    """
    Rebalance clusters and save them to files.
    
    Args:
        df: DataFrame to process
        output_folder: Where to save the output files
        threshold: Maximum threshold for cluster size
        min_threshold: Minimum threshold for cluster size
    """
    if df.empty:
        print("No data to process after filtering.")
        return
    
    batch_counter = 1
    
    # Check if 'content_labels' column exists
    if 'content_labels' in df.columns:
        # Get large content labels
        content_label_counts = df.groupby('content_labels').size().reset_index(name='count')
        content_label_counts['pairs'] = content_label_counts['count'] // 2
        
        # Identify large content labels (over threshold)
        large_labels = content_label_counts[content_label_counts['pairs'] > threshold]['content_labels'].tolist()
        print(f"Found {len(large_labels)} large content labels that need splitting")
        
        if large_labels:
            for label in large_labels:
                label_df = df[df['content_labels'] == label]
                label_pairs = len(label_df) // 2
                print(f"Processing large content label {label}: {label_pairs} pairs")
                
                # Calculate how many segments needed (size divide by threshold)
                num_segments = (label_pairs + threshold - 1) // threshold  # Ceiling division
                
                # Split and save
                split_and_save_cluster(label_df, output_folder, batch_counter, num_segments)
                batch_counter += num_segments
        
    # Also check for label column (as mentioned in point 3)
    if 'labels' in df.columns:
        label_counts = df.groupby('labels').size().reset_index(name='count')
        label_counts['pairs'] = label_counts['count'] // 2
        
        # Process by labels too
        for _, row in label_counts.iterrows():
            label = row['labels']
            pairs = row['pairs']
            
            if pairs > threshold:
                label_df = df[df['labels'] == label]
                print(f"Processing large label {label}: {pairs} pairs")
                
                # Check if this label has been processed via content_labels already
                if 'content_labels' in df.columns:
                    content_labels_in_this_label = label_df['content_labels'].unique()
                    already_processed = any(cl in large_labels for cl in content_labels_in_this_label)
                    
                    if already_processed:
                        print(f"Skipping label {label} as its content labels were already processed")
                        continue
                
                # Calculate segments
                num_segments = (pairs + threshold - 1) // threshold  # Ceiling division
                
                # Split and save
                split_and_save_cluster(label_df, output_folder, batch_counter, num_segments)
                batch_counter += num_segments
    
    # Process remaining data not covered by large content_labels or labels
    processed_mask = pd.Series(False, index=df.index)
    
    if 'content_labels' in df.columns and large_labels:
        for label in large_labels:
            processed_mask = processed_mask | (df['content_labels'] == label)
    
    if 'labels' in df.columns:
        for label in label_counts[label_counts['pairs'] > threshold]['labels']:
            # Only mark as processed if not already processed by content_labels
            if 'content_labels' in df.columns:
                label_df = df[df['labels'] == label]
                content_labels_in_this_label = label_df['content_labels'].unique()
                already_processed = any(cl in large_labels for cl in content_labels_in_this_label)
                
                if not already_processed:
                    processed_mask = processed_mask | (df['labels'] == label)
            else:
                processed_mask = processed_mask | (df['labels'] == label)
    
    remaining_df = df[~processed_mask]
    remaining_pairs = len(remaining_df) // 2
    
    if remaining_pairs > 0:
        print(f"Processing remaining data: {remaining_pairs} pairs")
        
        if remaining_pairs <= threshold:
            # Small enough to keep as is
            save_cluster(remaining_df, output_folder, batch_counter)
        else:
            # Split into appropriate segments
            num_segments = (remaining_pairs + threshold - 1) // threshold  # Ceiling division
            split_and_save_cluster(remaining_df, output_folder, batch_counter, num_segments)

def split_and_save_cluster(df, output_folder, start_batch_num, num_segments):
    """
    Split a cluster into specified number of segments and save them.
    
    Args:
        df: DataFrame to split
        output_folder: Where to save the output files
        start_batch_num: Starting batch number
        num_segments: Number of segments to split into
    """
    total_rows = len(df)
    segment_size = total_rows // num_segments
    
    for i in range(num_segments):
        start_idx = i * segment_size
        end_idx = (i + 1) * segment_size if i < num_segments - 1 else total_rows
        segment = df.iloc[start_idx:end_idx].copy()
        
        batch_num = start_batch_num + i
        save_cluster(segment, output_folder, batch_num)

def save_cluster(df, output_folder, batch_num):
    """
    Save a cluster to a JSON file.
    
    Args:
        df: DataFrame to save
        output_folder: Where to save the file
        batch_num: Batch number for file naming
    """
    output_file = os.path.join(output_folder, f"Batch_rebalanced_{batch_num}.json")
    df.to_json(path_or_buf=output_file, orient='index', double_precision=10, indent=4, date_format='iso')
    
    pair_count = len(df) // 2
    print(f"Saved Batch_rebalanced_{batch_num}.json: {pair_count} pairs")

if __name__ == "__main__":
    input_folder = "./input_json_files"
    output_folder = "./output_clusters"
    threshold = 70  # Passed as parameter as required in point 4
    min_threshold = 30
    
    print(f"Starting cluster rebalancing with threshold: {threshold} pairs")
    print(f"Input folder: {input_folder}")
    print(f"Output folder: {output_folder}")
    
    process_input_folder(input_folder, output_folder, threshold, min_threshold)
    
    print("Cluster rebalancing completed")
