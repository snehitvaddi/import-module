import os
import json
import pandas as pd

def process_input_folder(input_folder, output_folder, threshold=70, min_threshold=30):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    
    output_clusters = []
    batch_num = 1
    
    print(f"Processing JSON files from: {input_folder}")
    for filename in sorted(os.listdir(input_folder)):
        if filename.endswith('.json'):
            file_path = os.path.join(input_folder, filename)
            
            try:
                cluster_id = filename.split('.')[0]
                print(f"\nProcessing: {filename}")
                
                df = pd.read_json(path_or_buf=file_path, orient='index')
                
                if 'final_label' in df.columns:
                    df['original_cluster'] = cluster_id
                    # Do NOT modify final_label - keep original values to avoid duplicates
                
                filtered_df = df[df["final_sim_score"] > 0.92]
                
                pairs = len(filtered_df) // 2
                print(f"File: {filename}, Pairs: {pairs}, Threshold: {threshold}")
                
                # Verify no final_label violations in input
                if 'final_label' in filtered_df.columns:
                    final_label_counts = filtered_df.groupby('final_label').size()
                    max_count = final_label_counts.max() if not final_label_counts.empty else 0
                    if max_count > 2:
                        print(f"WARNING: Input file {filename} has final_label violations (max count: {max_count})")
                        print(f"Fixing by keeping at most 2 rows per final_label...")
                        filtered_df = enforce_final_label_constraint(filtered_df)
                        pairs = len(filtered_df) // 2
                        print(f"After fixing: {pairs} pairs")
                
                # Decide how to handle this file based on size
                if pairs <= threshold:
                    # File is within or below threshold - keep as single cluster
                    print(f"KEEPING intact: {pairs} pairs (within/below threshold)")
                    output_clusters.append((filtered_df, cluster_id))
                else:
                    # File exceeds threshold - split while respecting final_label constraint
                    print(f"SPLITTING: {pairs} pairs (exceeds threshold)")
                    num_segments = (pairs + threshold - 1) // threshold  # Ceiling division
                    
                    if 'final_label' in filtered_df.columns:
                        split_clusters = split_by_final_label(filtered_df, num_segments)
                    else:
                        split_clusters = split_cluster(filtered_df, num_segments)
                    
                    for i, segment in enumerate(split_clusters):
                        segment_pairs = len(segment) // 2
                        print(f"  - Split {i+1}/{len(split_clusters)}: {segment_pairs} pairs")
                        output_clusters.append((segment, f"{cluster_id}_split{i+1}"))
                    
            except Exception as e:
                print(f"Error processing {filename}: {e}")
    
    print("\nSaving all clusters...")
    
    # Save all clusters
    for cluster_df, source_id in output_clusters:
        save_cluster(cluster_df, output_folder, batch_num, source_id)
        batch_num += 1
    
    print("\nValidating final outputs...")
    
    # Final report
    saved_clusters_counts = {}
    final_label_violations = {}
    original_cluster_info = {}
    
    for filename in sorted(os.listdir(output_folder)):
        if filename.endswith(".json"):
            file_path = os.path.join(output_folder, filename)
            df_saved = pd.read_json(file_path, orient='index')
            saved_clusters_counts[filename] = len(df_saved) // 2
            
            if 'original_cluster' in df_saved.columns:
                original_clusters = df_saved['original_cluster'].unique()
                original_cluster_info[filename] = list(original_clusters)
            
            if 'final_label' in df_saved.columns:
                final_label_counts = df_saved.groupby('final_label').size()
                violations = final_label_counts[final_label_counts > 2]
                if not violations.empty:
                    final_label_violations[filename] = violations.to_dict()
    
    print(f"\nREBALANCING SUMMARY:")
    print(f"Total clusters created: {len(saved_clusters_counts)}")
    
    for filename, pair_count in sorted(saved_clusters_counts.items()):
        status = []
        if filename in final_label_violations:
            status.append(f"VIOLATION: {len(final_label_violations[filename])} final_labels")
        if pair_count < min_threshold:
            status.append(f"SMALL: below min threshold ({min_threshold})")
        elif pair_count > threshold:
            status.append(f"LARGE: above max threshold ({threshold})")
        
        if filename in original_cluster_info:
            if len(original_cluster_info[filename]) == 1:
                orig_info = f"from {original_cluster_info[filename][0]}"
            else:
                orig_info = f"merged from {', '.join(original_cluster_info[filename])}"
        else:
            orig_info = ""
        
        status_str = f" - {', '.join(status)}" if status else ""
        origin_str = f" ({orig_info})" if orig_info else ""
        print(f"{filename}: {pair_count} pairs{status_str}{origin_str}")
    
    if final_label_violations:
        print(f"\nWARNING: Found {len(final_label_violations)} clusters violating final_label constraint")
        for filename, violations in final_label_violations.items():
            print(f"  - {filename}: {violations}")
        print("This should never happen with this algorithm. Please report this as a bug.")

def enforce_final_label_constraint(df):
    """
    Ensure no more than 2 rows have the same final_label in the DataFrame.
    For each final_label with more than 2 occurrences, keep only the first 2.
    """
    if 'final_label' not in df.columns:
        return df
        
    # Check if constraint is already satisfied
    label_counts = df.groupby('final_label').size()
    if (label_counts <= 2).all():
        return df
        
    # Identify labels with more than 2 occurrences
    violations = label_counts[label_counts > 2]
    print(f"Found {len(violations)} final_label violations to fix")
    
    # Create a clean DataFrame with at most 2 rows per final_label
    result_df = pd.DataFrame(columns=df.columns)
    
    # Group by final_label and keep at most 2 rows from each group
    grouped = df.groupby('final_label')
    
    for label, group in grouped:
        if len(group) <= 2:
            # No violation for this label, keep all rows
            result_df = pd.concat([result_df, group], ignore_index=True)
        else:
            # Violation, keep only first 2 rows
            result_df = pd.concat([result_df, group.iloc[:2]], ignore_index=True)
            print(f"  - Kept 2/{len(group)} rows for label {label}")
    
    print(f"After enforcing constraint: {len(result_df)} rows (removed {len(df) - len(result_df)} rows)")
    return result_df

def split_cluster(df, num_segments):
    """Split a DataFrame into roughly equal segments."""
    total_rows = len(df)
    segment_size = total_rows // num_segments
    segments = []
    
    for i in range(num_segments):
        start_idx = i * segment_size
        end_idx = (i + 1) * segment_size if i < num_segments - 1 else total_rows
        segment = df.iloc[start_idx:end_idx].copy()
        segments.append(segment)
    
    return segments

def split_by_final_label(df, num_segments):
    """Split a DataFrame while respecting the final_label constraint."""
    if 'final_label' not in df.columns:
        return split_cluster(df, num_segments)
    
    # Ensure input has no violations
    df = enforce_final_label_constraint(df)
    
    # Get all unique final_labels
    unique_labels = df['final_label'].unique()
    
    # Initialize segments
    segments = [pd.DataFrame(columns=df.columns) for _ in range(num_segments)]
    
    # Group rows by final_label
    grouped = df.groupby('final_label')
    
    # Distribute final_labels across segments
    for i, (label, group) in enumerate(grouped):
        # Always keep all rows with the same final_label in the SAME segment
        # Assign to segment with minimum current size
        target_segment = min(range(len(segments)), key=lambda i: len(segments[i]))
        segments[target_segment] = pd.concat([segments[target_segment], group], ignore_index=True)
    
    # Remove any empty segments
    segments = [segment for segment in segments if not segment.empty]
    
    # Try to balance segment sizes
    if len(segments) > 1:
        min_size = min(len(segment) for segment in segments)
        max_size = max(len(segment) for segment in segments)
        
        if max_size - min_size > 20:
            print(f"  - Note: Segments are unbalanced (size range: {min_size//2}-{max_size//2} pairs)")
            print(f"  - This is expected when keeping final_labels together")
    
    # Report final sizes
    for i, segment in enumerate(segments):
        segment_label_counts = segment.groupby('final_label').size()
        max_count = segment_label_counts.max() if not segment_label_counts.empty else 0
        print(f"  - Segment {i+1}: {len(segment)//2} pairs, max final_label count: {max_count}")
    
    return segments

def save_cluster(df, output_folder, batch_num, source_id):
    """Save a cluster to a JSON file with verification."""
    # Final verification of constraint
    if 'final_label' in df.columns:
        final_label_counts = df.groupby('final_label').size()
        violations = final_label_counts[final_label_counts > 2]
        if not violations.empty:
            print(f"WARNING: Found violations before saving cluster {batch_num}, fixing...")
            df = enforce_final_label_constraint(df)
    
    output_file = os.path.join(output_folder, f"Batch_rebalanced_{batch_num}.json")
    df.to_json(path_or_buf=output_file, orient='index', double_precision=10, indent=4, date_format='iso')
    
    pair_count = len(df) // 2
    print(f"Saved Batch_rebalanced_{batch_num}.json: {pair_count} pairs (from {source_id})")

if __name__ == "__main__":
    input_folder = "./input_json_files"
    output_folder = "./output_clusters"
    threshold = 70
    min_threshold = 30
    
    print(f"Starting cluster rebalancing with threshold: {threshold} pairs")
    print(f"Input folder: {input_folder}")
    print(f"Output folder: {output_folder}")
    
    process_input_folder(input_folder, output_folder, threshold, min_threshold)
    
    print("Cluster rebalancing completed")
