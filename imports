import os
import json
import pandas as pd
import re

def process_input_folder(input_folder, output_folder, threshold=70, min_threshold=30, ideal_min=65, ideal_max=75):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    
    small_clusters = []
    valid_clusters = []
    
    print(f"Processing JSON files from: {input_folder}")
    for filename in sorted(os.listdir(input_folder)):
        if filename.endswith('.json'):
            file_path = os.path.join(input_folder, filename)
            
            try:
                match = re.search(r'C(\d+)', filename)
                if match:
                    cluster_id = f"C{match.group(1)}"
                else:
                    cluster_id = filename.split('.')[0]
                
                print(f"Extracted cluster ID '{cluster_id}' from filename '{filename}'")
                
                df = pd.read_json(path_or_buf=file_path, orient='index')
                
                if 'final_label' in df.columns:
                    df['original_cluster'] = cluster_id
                    # This is where we modify the final_label to include the cluster ID
                    df['final_label'] = df['final_label'].astype(str) + '_' + cluster_id
                
                filtered_df = df[df["final_sim_score"] > 0.92]
                
                pairs = len(filtered_df) // 2
                print(f"File: {filename}, Pairs: {pairs}, Threshold: {threshold}")
                
                if 'final_label' in filtered_df.columns:
                    final_label_counts = filtered_df.groupby('final_label').size()
                    max_count = final_label_counts.max() if not final_label_counts.empty else 0
                    if max_count > 2:
                        print(f"WARNING: Input file {filename} violates final_label constraint (max: {max_count})")
                        # Show the specific violating labels with their full modified values
                        violations = final_label_counts[final_label_counts > 2]
                        for label, count in violations.items():
                            print(f"  - Label '{label}' appears {count} times")
                
                if pairs <= threshold and pairs >= min_threshold:
                    print(f"KEEPING: {filename} is within size range ({pairs} pairs)")
                    valid_clusters.append(filtered_df)
                    
                elif pairs > threshold:
                    print(f"SPLITTING: {filename} exceeds threshold ({pairs} pairs)")
                    
                    ideal_size = (ideal_min + ideal_max) / 2
                    optimal_segments = round(pairs / ideal_size)
                    min_segments = (pairs + threshold - 1) // threshold
                    num_segments = max(optimal_segments, min_segments)
                    
                    print(f"  - Creating {num_segments} segments (targeting {ideal_size} pairs per segment)")
                    
                    if 'final_label' in filtered_df.columns:
                        split_clusters = split_by_final_label(filtered_df, num_segments)
                    else:
                        split_clusters = split_cluster(filtered_df, num_segments)
                    
                    for i, cluster in enumerate(split_clusters):
                        cluster_pairs = len(cluster) // 2
                        in_ideal_range = ideal_min <= cluster_pairs <= ideal_max
                        range_status = "IDEAL RANGE" if in_ideal_range else ""
                        print(f"  - Split {i+1}/{num_segments}: {cluster_pairs} pairs {range_status}")
                        
                        if cluster_pairs >= min_threshold:
                            valid_clusters.append(cluster)
                        else:
                            small_clusters.append(cluster)
                            print(f"    - Added to small clusters pool ({cluster_pairs} pairs)")
                    
                else:
                    print(f"SMALL CLUSTER: {filename} is below minimum threshold ({pairs} pairs)")
                    small_clusters.append(filtered_df)
                    
            except Exception as e:
                print(f"Error processing {filename}: {e}")
    
    merged_clusters = []
    if small_clusters:
        print(f"MERGING: Processing {len(small_clusters)} small clusters")
        merged_clusters = merge_small_clusters(small_clusters, min_threshold, ideal_min)
    
    all_clusters = valid_clusters + merged_clusters
    
    saved_count = 0
    for cluster in all_clusters:
        saved_count += 1
        save_cluster(cluster, output_folder, saved_count)
    
    saved_clusters_counts = {}
    final_label_violations = {}
    original_cluster_info = {}
    
    for filename in sorted(os.listdir(output_folder)):
        if filename.endswith(".json"):
            file_path = os.path.join(output_folder, filename)
            df_saved = pd.read_json(file_path, orient='index')
            saved_clusters_counts[filename] = len(df_saved) // 2
            
            if 'original_cluster' in df_saved.columns:
                original_clusters = df_saved['original_cluster'].unique()
                original_cluster_info[filename] = list(original_clusters)
            
            if 'final_label' in df_saved.columns:
                final_label_counts = df_saved.groupby('final_label').size()
                violations = final_label_counts[final_label_counts > 2]
                if not violations.empty:
                    final_label_violations[filename] = violations.to_dict()
    
    print(f"\nREBALANCING SUMMARY:")
    print(f"Total clusters created: {len(saved_clusters_counts)}")
    
    for filename, pair_count in sorted(saved_clusters_counts.items()):
        status = []
        if filename in final_label_violations:
            status.append(f"VIOLATION: {len(final_label_violations[filename])} final_labels")
        if pair_count < min_threshold:
            status.append(f"SMALL: below min threshold ({min_threshold})")
        elif pair_count > threshold:
            status.append(f"LARGE: above max threshold ({threshold})")
        elif ideal_min <= pair_count <= ideal_max:
            status.append(f"IDEAL: within ideal range ({ideal_min}-{ideal_max})")
        
        if filename in original_cluster_info:
            if len(original_cluster_info[filename]) == 1:
                orig_info = f"from {original_cluster_info[filename][0]}"
            else:
                orig_info = f"merged from {', '.join(original_cluster_info[filename])}"
        else:
            orig_info = ""
        
        status_str = f" - {', '.join(status)}" if status else ""
        origin_str = f" ({orig_info})" if orig_info else ""
        print(f"{filename}: {pair_count} pairs{status_str}{origin_str}")
    
    if final_label_violations:
        print(f"\nWARNING: Found {len(final_label_violations)} clusters violating final_label constraint")
        for filename, violations in final_label_violations.items():
            print(f"  - {filename} violations:")
            for label, count in violations.items():
                # Display the full modified final_label that includes the cluster ID
                print(f"    - Label '{label}' appears {count} times")

def split_cluster(df, num_segments):
    total_rows = len(df)
    segment_size = total_rows // num_segments
    segments = []
    
    for i in range(num_segments):
        start_idx = i * segment_size
        end_idx = (i + 1) * segment_size if i < num_segments - 1 else total_rows
        segment = df.iloc[start_idx:end_idx].copy()
        segments.append(segment)
    
    return segments

def split_by_final_label(df, num_segments):
    if 'final_label' not in df.columns:
        return split_cluster(df, num_segments)
    
    unique_labels = df['final_label'].unique()
    segments = [pd.DataFrame(columns=df.columns) for _ in range(num_segments)]
    
    for label in unique_labels:
        label_rows = df[df['final_label'] == label]
        
        if len(label_rows) <= 2:
            smallest_idx = min(range(len(segments)), key=lambda i: len(segments[i]))
            segments[smallest_idx] = pd.concat([segments[smallest_idx], label_rows], ignore_index=True)
        else:
            for i, row in enumerate(label_rows.itertuples()):
                segment_counts = [len(segments[j][segments[j]['final_label'] == label]) for j in range(len(segments))]
                target_segment = segment_counts.index(min(segment_counts))
                
                if segment_counts[target_segment] >= 2:
                    target_segment = min(range(len(segments)), key=lambda j: len(segments[j]))
                
                row_dict = {col: getattr(row, col) for col in df.columns if hasattr(row, col)}
                segments[target_segment] = pd.concat([segments[target_segment], pd.DataFrame([row_dict])], ignore_index=True)
    
    segments = [segment for segment in segments if not segment.empty]
    
    if len(segments) > 1:
        min_size = min(len(segment) for segment in segments)
        max_size = max(len(segment) for segment in segments)
        
        if max_size - min_size > 10:
            print(f"  - Balancing segments (size range: {min_size}-{max_size})")
            segments.sort(key=len, reverse=True)
            
            for i in range(len(segments) - 1):
                if len(segments[i]) - len(segments[-1]) <= 5:
                    break
                
                for idx, row in segments[i].iterrows():
                    label = row['final_label']
                    if len(segments[-1][segments[-1]['final_label'] == label]) < 2:
                        segments[-1] = pd.concat([segments[-1], row.to_frame().T], ignore_index=True)
                        segments[i] = segments[i].drop(idx)
                        break
    
    return segments

def merge_small_clusters(clusters, min_threshold, ideal_min=65):
    if not clusters:
        return []
    
    clusters_with_pairs = [(cluster, len(cluster) // 2) for cluster in clusters]
    sorted_clusters = sorted(clusters_with_pairs, key=lambda x: x[1], reverse=True)
    
    if len(sorted_clusters) == 1:
        print(f"Only one small cluster ({sorted_clusters[0][1]} pairs), keeping as is")
        return [sorted_clusters[0][0]]
    
    merged_clusters = []
    remaining = sorted_clusters.copy()
    
    while len(remaining) >= 2:
        base_cluster, base_pairs = remaining.pop(0)
        
        best_partner = None
        best_partner_idx = -1
        best_combined_pairs = 0
        best_match_score = 0
        
        for i, (partner, partner_pairs) in enumerate(remaining):
            combined_pairs = base_pairs + partner_pairs
            
            if combined_pairs > min_threshold * 2:
                continue
            
            if combined_pairs < ideal_min:
                match_score = combined_pairs / ideal_min
            else:
                match_score = 1.0
                
            if 'final_label' in base_cluster.columns and 'final_label' in partner.columns:
                base_counts = base_cluster.groupby('final_label').size().to_dict()
                partner_counts = partner.groupby('final_label').size().to_dict()
                
                violation = False
                for label, count in base_counts.items():
                    if label in partner_counts and count + partner_counts[label] > 2:
                        violation = True
                        break
                
                if violation:
                    continue
            
            if match_score > best_match_score:
                best_partner = partner
                best_partner_idx = i
                best_combined_pairs = combined_pairs
                best_match_score = match_score
        
        if best_partner is not None:
            merged = pd.concat([base_cluster, best_partner], ignore_index=True)
            merged_pairs = len(merged) // 2
            
            ideal_range_status = ""
            if merged_pairs >= ideal_min:
                ideal_range_status = " (in ideal range!)"
            
            print(f"MERGED: {base_pairs} pairs + {remaining[best_partner_idx][1]} pairs = {merged_pairs} pairs{ideal_range_status}")
            
            remaining.pop(best_partner_idx)
            
            if merged_pairs >= min_threshold:
                merged_clusters.append(merged)
            else:
                remaining.append((merged, merged_pairs))
                remaining.sort(key=lambda x: x[1], reverse=True)
        else:
            print(f"No compatible merge partner for cluster with {base_pairs} pairs")
            merged_clusters.append(base_cluster)
    
    for cluster, pairs in remaining:
        print(f"Keeping remaining unmerged cluster with {pairs} pairs")
        merged_clusters.append(cluster)
    
    return merged_clusters

def save_cluster(df, output_folder, batch_num):
    output_file = os.path.join(output_folder, f"Batch_rebalanced_{batch_num}.json")
    df.to_json(path_or_buf=output_file, orient='index', double_precision=10, indent=4, date_format='iso')
    
    pair_count = len(df) // 2
    
    if 'final_label' in df.columns:
        final_label_counts = df.groupby('final_label').size()
        max_count = final_label_counts.max() if not final_label_counts.empty else 0
        violates_constraint = (final_label_counts > 2).any()
        
        if violates_constraint:
            print(f"Saved Batch_rebalanced_{batch_num}.json: {pair_count} pairs - VIOLATES final_label constraint (max count: {max_count})")
            # Show the specific violating labels with their full modified values
            violations = final_label_counts[final_label_counts > 2]
            for label, count in violations.items():
                print(f"  - Label '{label}' appears {count} times")
        else:
            print(f"Saved Batch_rebalanced_{batch_num}.json: {pair_count} pairs")
    else:
        print(f"Saved Batch_rebalanced_{batch_num}.json: {pair_count} pairs")

if __name__ == "__main__":
    input_folder = "./input_json_files"
    output_folder = "./output_clusters"
    threshold = 70  # Maximum cluster size (pairs)
    min_threshold = 30  # Minimum cluster size (pairs)
    ideal_min = 65  # Minimum of ideal range (pairs)
    ideal_max = 75  # Maximum of ideal range (pairs)
    
    print(f"Starting cluster rebalancing with threshold: {threshold} pairs")
    print(f"Ideal range: {ideal_min}-{ideal_max} pairs")
    print(f"Input folder: {input_folder}")
    print(f"Output folder: {output_folder}")
    
    process_input_folder(input_folder, output_folder, threshold, min_threshold, ideal_min, ideal_max)
    
    print("Cluster rebalancing completed")
