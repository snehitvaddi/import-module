import os
import json
import pandas as pd
import re
from collections import defaultdict

def process_input_folder(input_folder, output_folder, threshold=70, min_threshold=30, ideal_min=65, ideal_max=75):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    
    small_clusters = []
    valid_clusters = []
    
    # Track total row counts
    total_input_rows = 0
    processed_files = 0
    
    print(f"Processing JSON files from: {input_folder}")
    for filename in sorted(os.listdir(input_folder)):
        if filename.endswith('.json'):
            file_path = os.path.join(input_folder, filename)
            processed_files += 1
            
            try:
                # Enhanced pattern matching for cluster IDs
                # Try pattern like "_C5" specifically for files like "Batch_20250501_C5_chunk.json"
                match = re.search(r'_C(\d+)', filename)
                if match:
                    cluster_id = f"C{match.group(1)}"
                else:
                    # Try standard "C1" pattern next
                    match = re.search(r'C(\d+)', filename)
                    if match:
                        cluster_id = f"C{match.group(1)}"
                    else:
                        # Fallback to original method if pattern not found
                        cluster_id = filename.split('.')[0]  # Remove the .json extension
                
                print(f"Extracted cluster ID '{cluster_id}' from filename '{filename}'")
                
                # Read the JSON file as a DataFrame
                df = pd.read_json(path_or_buf=file_path, orient='index')
                total_input_rows += len(df)
                
                # Sample a few final_labels before modification for logging
                original_labels = []
                if 'final_label' in df.columns and not df.empty:
                    try:
                        original_labels = list(df['final_label'].iloc[:3])
                    except Exception as e:
                        print(f"  - Warning: Could not sample original labels: {e}")
                    
                # Add original cluster identifier to final_label if it exists
                if 'final_label' in df.columns:
                    # Create a new column with original_cluster info
                    df['original_cluster'] = cluster_id
                    
                    # FIXED: First ensure all final_labels are properly converted to strings
                    df['final_label'] = df['final_label'].astype(str)
                    
                    # Modify final_label to include original cluster
                    print(f"  - Modifying final_labels to include cluster ID '{cluster_id}'")
                    df['final_label'] = df['final_label'] + '_' + cluster_id
                    
                    # Sample the modified labels to confirm change
                    if not df.empty:
                        try:
                            modified_labels = list(df['final_label'].iloc[:3])
                            print(f"  - Example labels before: {original_labels}")
                            print(f"  - Example labels after:  {modified_labels}")
                        except Exception as e:
                            print(f"  - Warning: Could not sample modified labels: {e}")
                
                filtered_df = df[df["final_sim_score"] > 0.92]
                
                pairs = len(filtered_df) // 2
                print(f"File: {filename}, Pairs: {pairs}, Threshold: {threshold}")
                
                if 'final_label' in filtered_df.columns:
                    try:
                        final_label_counts = filtered_df.groupby('final_label').size()
                        max_count = final_label_counts.max() if not final_label_counts.empty else 0
                        if max_count > 2:
                            print(f"WARNING: Input file {filename} violates final_label constraint (max: {max_count})")
                            violations = final_label_counts[final_label_counts > 2]
                            for label, count in violations.items():
                                print(f"  - Modified label '{label}' appears {count} times")
                    except Exception as e:
                        print(f"  - Warning: Could not check for violations: {e}")
                
                if pairs <= threshold and pairs >= min_threshold:
                    print(f"KEEPING: {filename} is within size range ({pairs} pairs)")
                    valid_clusters.append(filtered_df)
                    
                elif pairs > threshold:
                    print(f"SPLITTING: {filename} exceeds threshold ({pairs} pairs)")
                    
                    ideal_size = (ideal_min + ideal_max) / 2
                    optimal_segments = round(pairs / ideal_size)
                    min_segments = (pairs + threshold - 1) // threshold
                    num_segments = max(optimal_segments, min_segments)
                    
                    print(f"  - Creating {num_segments} segments (targeting {ideal_size} pairs per segment)")
                    
                    if 'final_label' in filtered_df.columns:
                        split_clusters = split_by_final_label(filtered_df, num_segments)
                    else:
                        split_clusters = split_cluster(filtered_df, num_segments)
                    
                    for i, cluster in enumerate(split_clusters):
                        cluster_pairs = len(cluster) // 2
                        in_ideal_range = ideal_min <= cluster_pairs <= ideal_max
                        range_status = "IDEAL RANGE" if in_ideal_range else ""
                        print(f"  - Split {i+1}/{num_segments}: {cluster_pairs} pairs {range_status}")
                        
                        if cluster_pairs >= min_threshold:
                            valid_clusters.append(cluster)
                        else:
                            small_clusters.append(cluster)
                            print(f"    - Added to small clusters pool ({cluster_pairs} pairs)")
                    
                else:
                    print(f"SMALL CLUSTER: {filename} is below minimum threshold ({pairs} pairs)")
                    small_clusters.append(filtered_df)
                    
            except Exception as e:
                print(f"Error processing {filename}: {e}")
    
    merged_clusters = []
    if small_clusters:
        print(f"MERGING: Processing {len(small_clusters)} small clusters")
        merged_clusters = merge_small_clusters(small_clusters, min_threshold, ideal_min)
    
    all_clusters = valid_clusters + merged_clusters
    
    # NEW: Fix final_label violations before saving
    print("\n===== FIXING FINAL_LABEL VIOLATIONS =====")
    all_clusters = fix_final_label_violations(all_clusters)
    
    saved_count = 0
    total_output_rows = 0
    
    for cluster in all_clusters:
        saved_count += 1
        total_output_rows += len(cluster)
        save_cluster(cluster, output_folder, saved_count)
    
    saved_clusters_counts = {}
    final_label_violations = {}
    original_cluster_info = {}
    
    for filename in sorted(os.listdir(output_folder)):
        if filename.endswith(".json"):
            file_path = os.path.join(output_folder, filename)
            try:
                df_saved = pd.read_json(file_path, orient='index')
                saved_clusters_counts[filename] = len(df_saved) // 2
                
                if 'original_cluster' in df_saved.columns:
                    original_clusters = df_saved['original_cluster'].unique()
                    original_cluster_info[filename] = list(original_clusters)
                
                if 'final_label' in df_saved.columns:
                    try:
                        # Convert all final_labels to strings for consistent handling
                        df_saved['final_label'] = df_saved['final_label'].astype(str)
                        final_label_counts = df_saved.groupby('final_label').size()
                        violations = final_label_counts[final_label_counts > 2]
                        if not violations.empty:
                            final_label_violations[filename] = violations.to_dict()
                    except Exception as e:
                        print(f"Error checking violations in {filename}: {e}")
            except Exception as e:
                print(f"Error reading saved file {filename}: {e}")
    
    print(f"\nREBALANCING SUMMARY:")
    print(f"Total input files processed: {processed_files}")
    print(f"Total input rows: {total_input_rows}")
    print(f"Total output rows: {total_output_rows}")
    print(f"Total input pairs: {total_input_rows // 2}")
    print(f"Total output pairs: {total_output_rows // 2}")
    print(f"Total clusters created: {len(saved_clusters_counts)}")
    
    for filename, pair_count in sorted(saved_clusters_counts.items()):
        status = []
        if filename in final_label_violations:
            status.append(f"VIOLATION: {len(final_label_violations[filename])} final_labels")
        if pair_count < min_threshold:
            status.append(f"SMALL: below min threshold ({min_threshold})")
        elif pair_count > threshold:
            status.append(f"LARGE: above max threshold ({threshold})")
        elif ideal_min <= pair_count <= ideal_max:
            status.append(f"IDEAL: within ideal range ({ideal_min}-{ideal_max})")
        
        if filename in original_cluster_info:
            if len(original_cluster_info[filename]) == 1:
                orig_info = f"from {original_cluster_info[filename][0]}"
            else:
                orig_info = f"merged from {', '.join(original_cluster_info[filename])}"
        else:
            orig_info = ""
        
        status_str = f" - {', '.join(status)}" if status else ""
        origin_str = f" ({orig_info})" if orig_info else ""
        print(f"{filename}: {pair_count} pairs{status_str}{origin_str}")
    
    if final_label_violations:
        print(f"\n===== FINAL_LABEL CONSTRAINT VIOLATIONS =====")
        print(f"Found {len(final_label_violations)} clusters with violations")
        print(f"Note: Each final_label shown below includes the original cluster ID as a suffix")
        print(f"Format: 'original_label_cluster_id'")
        print(f"-------------------------------------------")
        
        for filename, violations in final_label_violations.items():
            print(f"File: {filename}")
            
            # Read the file to check labels with improved error handling
            file_path = os.path.join(output_folder, filename)
            try:
                df = pd.read_json(file_path, orient='index')
                if 'final_label' in df.columns:
                    # Convert to list with error handling for non-iterables
                    try:
                        # Force conversion to strings first
                        df['final_label'] = df['final_label'].astype(str)
                        sample_labels = list(df['final_label'].unique())[:5]
                        print(f"  Sample labels from file: {sample_labels}")
                    except Exception as e:
                        print(f"  Error getting sample labels: {e}")
            except Exception as e:
                print(f"  Error reading file for sample labels: {e}")
            
            # Process violations with enhanced error handling
            for full_label, count in violations.items():
                try:
                    print(f"  - Violation: '{full_label}' appears {count} times (exceeds limit of 2)")
                    
                    # Handle different data types safely
                    full_label_str = str(full_label)
                    if '_' in full_label_str:
                        parts = full_label_str.rsplit('_', 1)
                        if len(parts) == 2:
                            orig_label, cluster_id = parts
                            print(f"    (Original label '{orig_label}' from cluster '{cluster_id}')")
                except Exception as e:
                    print(f"  - Error processing violation for label: {e}")
        
        print(f"-------------------------------------------")

def fix_final_label_violations(clusters):
    """
    Fix final_label violations by redistributing records between clusters.
    
    Args:
        clusters: List of DataFrame clusters
        
    Returns:
        List of fixed DataFrame clusters
    """
    if not clusters:
        return []
    
    # Check which clusters have violations
    violating_clusters = []
    clean_clusters = []
    
    for i, cluster in enumerate(clusters):
        try:
            if 'final_label' not in cluster.columns:
                clean_clusters.append(cluster)
                continue
            
            # Ensure final_labels are converted to strings for consistent handling
            cluster['final_label'] = cluster['final_label'].astype(str)
                
            final_label_counts = cluster.groupby('final_label').size()
            violations = final_label_counts[final_label_counts > 2]
            
            if violations.empty:
                clean_clusters.append(cluster)
            else:
                print(f"Found cluster with {len(violations)} final_label violations (cluster size: {len(cluster)})")
                violating_clusters.append((i, cluster, violations))
        except Exception as e:
            print(f"Error checking for violations in cluster {i}: {e}")
            # If there was an error checking violations, treat it as clean to be safe
            clean_clusters.append(cluster)
    
    if not violating_clusters:
        print("No final_label violations found in any clusters")
        return clusters
    
    print(f"Found {len(violating_clusters)} clusters with violations. Attempting to fix...")
    
    # Records that need new homes
    orphaned_records = []
    
    # Process each violating cluster
    for original_idx, cluster, violations in violating_clusters:
        print(f"Processing cluster #{original_idx} with {len(violations)} violations")
        
        # Extract violating records
        for label, count in violations.items():
            try:
                # We need to remove (count - 2) records with this label
                records_to_remove = count - 2
                print(f"  Need to relocate {records_to_remove} occurrences of label '{label}'")
                
                # Get indices of rows with this label
                label_rows = cluster[cluster['final_label'] == label]
                
                # Remove the excess records (keep the first 2)
                excess_records = label_rows.iloc[2:2+records_to_remove]
                
                # Add to orphaned records
                for idx, row in excess_records.iterrows():
                    orphaned_records.append(row.to_dict())  # Convert to dict for consistent handling
                
                # Remove from original cluster
                cluster = cluster.drop(excess_records.index)
            except Exception as e:
                print(f"  Error processing violation for label '{label}': {e}")
        
        # Update the cluster in the clean list
        clean_clusters.append(cluster)
    
    print(f"Extracted {len(orphaned_records)} records that violate constraints")
    
    if not orphaned_records:
        return clean_clusters
    
    # Convert orphaned records to DataFrame with error handling
    try:
        orphan_df = pd.DataFrame(orphaned_records)
        
        # Ensure final_label is string type
        if 'final_label' in orphan_df.columns:
            orphan_df['final_label'] = orphan_df['final_label'].astype(str)
        
        # Group orphans by final_label
        orphans_by_label = defaultdict(list)
        for idx, row in orphan_df.iterrows():
            label = row.get('final_label', '')
            orphans_by_label[label].append(row.to_dict())
        
        print(f"Orphaned records have {len(orphans_by_label)} distinct final_labels")
    except Exception as e:
        print(f"Error preparing orphaned records: {e}")
        # If we had an error processing orphans, skip the redistribution
        return clean_clusters
    
    # Try to place orphans in clean clusters
    relocated_count = 0
    remaining_orphans = []
    
    for label, records in orphans_by_label.items():
        print(f"Finding homes for {len(records)} records with label '{label}'")
        
        for record in records:
            placed = False
            
            # Try each clean cluster
            for i, cluster in enumerate(clean_clusters):
                try:
                    if 'final_label' not in cluster.columns:
                        continue
                    
                    # Ensure consistent string comparison
                    if 'final_label' in cluster.columns:
                        cluster['final_label'] = cluster['final_label'].astype(str)
                        
                    # Count occurrences of this label in the cluster
                    label_count = len(cluster[cluster['final_label'] == label])
                    
                    # Can we add this record without violating constraint?
                    if label_count < 2:
                        # Yes, add it
                        clean_clusters[i] = pd.concat([cluster, pd.DataFrame([record])], ignore_index=True)
                        placed = True
                        relocated_count += 1
                        print(f"  Placed record with label '{label}' in cluster #{i} (now has {label_count + 1})")
                        break
                except Exception as e:
                    print(f"  Error checking cluster {i} for placement: {e}")
            
            if not placed:
                remaining_orphans.append(record)
    
    print(f"Successfully relocated {relocated_count} records")
    
    # Handle any remaining orphans
    if remaining_orphans:
        print(f"Creating new clusters for {len(remaining_orphans)} records that couldn't be relocated")
        
        try:
            # Group remaining orphans by label
            orphans_by_label = defaultdict(list)
            for record in remaining_orphans:
                label = str(record.get('final_label', ''))
                orphans_by_label[label].append(record)
            
            # Create new mini-clusters, ensuring no more than 2 records per label
            mini_clusters = []
            current_cluster = []
            
            # Keep track of labels in the current cluster
            labels_in_cluster = defaultdict(int)
            
            for label, records in orphans_by_label.items():
                for record in records:
                    # If adding this record would violate constraint, start a new cluster
                    if labels_in_cluster[label] >= 2:
                        if current_cluster:
                            mini_clusters.append(pd.DataFrame(current_cluster))
                            current_cluster = []
                            labels_in_cluster = defaultdict(int)
                    
                    # Add record to current cluster
                    current_cluster.append(record)
                    labels_in_cluster[label] += 1
            
            # Add the last cluster if not empty
            if current_cluster:
                mini_clusters.append(pd.DataFrame(current_cluster))
            
            print(f"Created {len(mini_clusters)} new small clusters for orphaned records")
            
            # Add mini-clusters to clean clusters
            clean_clusters.extend(mini_clusters)
        except Exception as e:
            print(f"Error creating mini-clusters: {e}")
    
    return clean_clusters

def split_cluster(df, num_segments):
    total_rows = len(df)
    segment_size = total_rows // num_segments
    segments = []
    
    for i in range(num_segments):
        start_idx = i * segment_size
        end_idx = (i + 1) * segment_size if i < num_segments - 1 else total_rows
        segment = df.iloc[start_idx:end_idx].copy()
        segments.append(segment)
    
    return segments

def split_by_final_label(df, num_segments):
    if 'final_label' not in df.columns:
        return split_cluster(df, num_segments)
    
    # Ensure final_label is string type for consistent handling
    df['final_label'] = df['final_label'].astype(str)
    
    unique_labels = df['final_label'].unique()
    segments = [pd.DataFrame(columns=df.columns) for _ in range(num_segments)]
    
    for label in unique_labels:
        label_rows = df[df['final_label'] == label]
        
        if len(label_rows) <= 2:
            smallest_idx = min(range(len(segments)), key=lambda i: len(segments[i]))
            segments[smallest_idx] = pd.concat([segments[smallest_idx], label_rows], ignore_index=True)
        else:
            for i, row in enumerate(label_rows.itertuples()):
                segment_counts = [len(segments[j][segments[j]['final_label'] == label]) for j in range(len(segments))]
                target_segment = segment_counts.index(min(segment_counts))
                
                if segment_counts[target_segment] >= 2:
                    target_segment = min(range(len(segments)), key=lambda j: len(segments[j]))
                
                row_dict = {col: getattr(row, col) for col in df.columns if hasattr(row, col)}
                segments[target_segment] = pd.concat([segments[target_segment], pd.DataFrame([row_dict])], ignore_index=True)
    
    segments = [segment for segment in segments if not segment.empty]
    
    if len(segments) > 1:
        min_size = min(len(segment) for segment in segments)
        max_size = max(len(segment) for segment in segments)
        
        if max_size - min_size > 10:
            print(f"  - Balancing segments (size range: {min_size}-{max_size})")
            segments.sort(key=len, reverse=True)
            
            for i in range(len(segments) - 1):
                if len(segments[i]) - len(segments[-1]) <= 5:
                    break
                
                for idx, row in segments[i].iterrows():
                    label = row['final_label']
                    if len(segments[-1][segments[-1]['final_label'] == label]) < 2:
                        segments[-1] = pd.concat([segments[-1], row.to_frame().T], ignore_index=True)
                        segments[i] = segments[i].drop(idx)
                        break
    
    return segments

def merge_small_clusters(clusters, min_threshold, ideal_min=65):
    if not clusters:
        return []
    
    clusters_with_pairs = [(cluster, len(cluster) // 2) for cluster in clusters]
    sorted_clusters = sorted(clusters_with_pairs, key=lambda x: x[1], reverse=True)
    
    if len(sorted_clusters) == 1:
        print(f"Only one small cluster ({sorted_clusters[0][1]} pairs), keeping as is")
        return [sorted_clusters[0][0]]
    
    merged_clusters = []
    remaining = sorted_clusters.copy()
    
    while len(remaining) >= 2:
        base_cluster, base_pairs = remaining.pop(0)
        
        best_partner = None
        best_partner_idx = -1
        best_combined_pairs = 0
        best_match_score = 0
        
        for i, (partner, partner_pairs) in enumerate(remaining):
            combined_pairs = base_pairs + partner_pairs
            
            if combined_pairs > min_threshold * 2:
                continue
            
            if combined_pairs < ideal_min:
                match_score = combined_pairs / ideal_min
            else:
                match_score = 1.0
                
            if 'final_label' in base_cluster.columns and 'final_label' in partner.columns:
                try:
                    # Ensure final_labels are string type for consistent comparison
                    base_cluster['final_label'] = base_cluster['final_label'].astype(str)
                    partner['final_label'] = partner['final_label'].astype(str)
                    
                    base_counts = base_cluster.groupby('final_label').size().to_dict()
                    partner_counts = partner.groupby('final_label').size().to_dict()
                    
                    violation = False
                    for label, count in base_counts.items():
                        if label in partner_counts and count + partner_counts[label] > 2:
                            violation = True
                            break
                    
                    if violation:
                        continue
                except Exception as e:
                    print(f"  Error checking constraint for potential merge: {e}")
                    continue
            
            if match_score > best_match_score:
                best_partner = partner
                best_partner_idx = i
                best_combined_pairs = combined_pairs
                best_match_score = match_score
        
        if best_partner is not None:
            try:
                merged = pd.concat([base_cluster, best_partner], ignore_index=True)
                merged_pairs = len(merged) // 2
                
                ideal_range_status = ""
                if merged_pairs >= ideal_min:
                    ideal_range_status = " (in ideal range!)"
                
                print(f"MERGED: {base_pairs} pairs + {remaining[best_partner_idx][1]} pairs = {merged_pairs} pairs{ideal_range_status}")
                
                remaining.pop(best_partner_idx)
                
                if merged_pairs >= min_threshold:
                    merged_clusters.append(merged)
                else:
                    remaining.append((merged, merged_pairs))
                    remaining.sort(key=lambda x: x[1], reverse=True)
            except Exception as e:
                print(f"Error merging clusters: {e}")
                # If merge failed, keep both clusters separate
                merged_clusters.append(base_cluster)
                merged_clusters.append(best_partner)
                remaining.pop(best_partner_idx)
        else:
            print(f"No compatible merge partner for cluster with {base_pairs} pairs")
            merged_clusters.append(base_cluster)
    
    for cluster, pairs in remaining:
        print(f"Keeping remaining unmerged cluster with {pairs} pairs")
        merged_clusters.append(cluster)
    
    return merged_clusters

def save_cluster(df, output_folder, batch_num):
    output_file = os.path.join(output_folder, f"Batch_rebalanced_{batch_num}.json")
    
    # Before saving, check if final_label is properly formatted
    if 'final_label' in df.columns:
        try:
            # Ensure all final_labels are strings for consistent handling
            df['final_label'] = df['final_label'].astype(str)
            
            # Count how many have underscores (should have cluster IDs)
            with_underscore = df['final_label'].str.contains('_').sum()
            print(f"  - {with_underscore} out of {len(df)} final_labels contain '_' (should indicate cluster ID)")
            
            # Sample a few labels for inspection
            if not df.empty:
                sample_labels = list(df['final_label'].iloc[:3])
                print(f"  - Sample final_labels: {sample_labels}")
        except Exception as e:
            print(f"  - Warning: Could not verify final_labels: {e}")
    
    try:
        df.to_json(path_or_buf=output_file, orient='index', double_precision=10, indent=4, date_format='iso')
        
        pair_count = len(df) // 2
        
        if 'final_label' in df.columns:
            # Ensure final_labels are strings
            df['final_label'] = df['final_label'].astype(str)
            final_label_counts = df.groupby('final_label').size()
            max_count = final_label_counts.max() if not final_label_counts.empty else 0
            violates_constraint = (final_label_counts > 2).any()
            
            if violates_constraint:
                print(f"Saved Batch_rebalanced_{batch_num}.json: {pair_count} pairs - VIOLATES final_label constraint (max count: {max_count})")
                violations = final_label_counts[final_label_counts > 2]
                print(f"  Violations (full modified labels including cluster ID):")
                for full_label, count in violations.items():
                    print(f"  - '{full_label}' appears {count} times (exceeds limit of 2)")
            else:
                print(f"Saved Batch_rebalanced_{batch_num}.json: {pair_count} pairs")
        else:
            print(f"Saved Batch_rebalanced_{batch_num}.json: {pair_count} pairs")
    except Exception as e:
        print(f"Error saving cluster to {output_file}: {e}")

if __name__ == "__main__":
    input_folder = "./input_json_files"
    output_folder = "./output_clusters"
    threshold = 70
    min_threshold = 30
    ideal_min = 65
    ideal_max = 75
    
    print(f"Starting cluster rebalancing with threshold: {threshold} pairs")
    print(f"Ideal range: {ideal_min}-{ideal_max} pairs")
    print(f"Input folder: {input_folder}")
    print(f"Output folder: {output_folder}")
    
    process_input_folder(input_folder, output_folder, threshold, min_threshold, ideal_min, ideal_max)
    
    print("Cluster rebalancing completed")
