import os
import json
import pandas as pd

def process_input_folder(input_folder, output_folder, threshold=70, min_threshold=30):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    
    all_dfs = []
    
    print(f"Processing JSON files from: {input_folder}")
    for filename in sorted(os.listdir(input_folder)):
        if filename.endswith('.json'):
            file_path = os.path.join(input_folder, filename)
            
            try:
                # Extract cluster identifier from filename
                import re
                cluster_match = re.search(r'_C(\d+)', filename)
                if cluster_match:
                    cluster_id = 'C' + cluster_match.group(1)
                else:
                    cluster_id = filename.split('.')[0]  # Use the whole name if pattern not found
                
                df = pd.read_json(path_or_buf=file_path, orient='index')
                
                if 'final_label' in df.columns:
                    df['original_cluster'] = cluster_id
                    df['final_label'] = df['final_label'].astype(str) + '_' + cluster_id
                
                filtered_df = df[df["final_sim_score"] > 0.92]
                
                pairs = len(filtered_df) // 2
                print(f"File: {filename}, Pairs: {pairs}, Threshold: {threshold}")
                
                # Check and fix final_label constraint in input file
                if 'final_label' in filtered_df.columns:
                    filtered_df = enforce_final_label_constraint(filtered_df)
                
                all_dfs.append(filtered_df)
                    
            except Exception as e:
                print(f"Error processing {filename}: {e}")
    
    # Combine all data into one DataFrame
    if not all_dfs:
        print("No valid data found in input files.")
        return
        
    combined_df = pd.concat(all_dfs, ignore_index=True)
    print(f"Combined data: {len(combined_df)} rows, {len(combined_df)//2} pairs")
    
    # Ensure final_label constraint is satisfied in combined data
    if 'final_label' in combined_df.columns:
        combined_df = enforce_final_label_constraint(combined_df)
    
    # Rebalance the combined data
    clusters = rebalance_combined_data(combined_df, threshold, min_threshold)
    
    # Save all clusters
    saved_count = 0
    for cluster in clusters:
        saved_count += 1
        # Verify final_label constraint before saving
        if 'final_label' in cluster.columns:
            final_label_counts = cluster.groupby('final_label').size()
            if (final_label_counts > 2).any():
                print(f"WARNING: Found violations in cluster {saved_count} before saving, fixing...")
                cluster = enforce_final_label_constraint(cluster)
        save_cluster(cluster, output_folder, saved_count)
    
    # Final report
    saved_clusters_counts = {}
    final_label_violations = {}
    original_cluster_info = {}
    
    for filename in sorted(os.listdir(output_folder)):
        if filename.endswith(".json"):
            file_path = os.path.join(output_folder, filename)
            df_saved = pd.read_json(file_path, orient='index')
            saved_clusters_counts[filename] = len(df_saved) // 2
            
            if 'original_cluster' in df_saved.columns:
                original_clusters = df_saved['original_cluster'].unique()
                original_cluster_info[filename] = list(original_clusters)
            
            if 'final_label' in df_saved.columns:
                final_label_counts = df_saved.groupby('final_label').size()
                violations = final_label_counts[final_label_counts > 2]
                if not violations.empty:
                    final_label_violations[filename] = violations.to_dict()
    
    print(f"\nREBALANCING SUMMARY:")
    print(f"Total clusters created: {len(saved_clusters_counts)}")
    
    for filename, pair_count in sorted(saved_clusters_counts.items()):
        status = []
        if filename in final_label_violations:
            status.append(f"VIOLATION: {len(final_label_violations[filename])} final_labels")
        if pair_count < min_threshold:
            status.append(f"SMALL: below min threshold ({min_threshold})")
        elif pair_count > threshold:
            status.append(f"LARGE: above max threshold ({threshold})")
        
        if filename in original_cluster_info:
            if len(original_cluster_info[filename]) == 1:
                orig_info = f"from {original_cluster_info[filename][0]}"
            else:
                orig_info = f"merged from {', '.join(original_cluster_info[filename])}"
        else:
            orig_info = ""
        
        status_str = f" - {', '.join(status)}" if status else ""
        origin_str = f" ({orig_info})" if orig_info else ""
        print(f"{filename}: {pair_count} pairs{status_str}{origin_str}")
    
    if final_label_violations:
        print(f"\nWARNING: Found {len(final_label_violations)} clusters violating final_label constraint")
        for filename, violations in final_label_violations.items():
            print(f"  - {filename}: {violations}")
        print("This should never happen with the current algorithm. Please report this bug.")

def enforce_final_label_constraint(df):
    """
    Ensure no more than 2 rows have the same final_label in the DataFrame.
    For each final_label with more than 2 occurrences, keep only the first 2.
    
    Args:
        df: DataFrame to process
        
    Returns:
        DataFrame with at most 2 rows per final_label
    """
    if 'final_label' not in df.columns:
        return df
        
    # Check if constraint is already satisfied
    label_counts = df.groupby('final_label').size()
    if (label_counts <= 2).all():
        return df
        
    # Identify labels with more than 2 occurrences
    violations = label_counts[label_counts > 2]
    print(f"Found {len(violations)} final_label violations to fix")
    
    # Create a clean DataFrame with at most 2 rows per final_label
    result_df = pd.DataFrame(columns=df.columns)
    
    # Group by final_label and keep at most 2 rows from each group
    grouped = df.groupby('final_label')
    
    for label, group in grouped:
        if len(group) <= 2:
            # No violation for this label, keep all rows
            result_df = pd.concat([result_df, group], ignore_index=True)
        else:
            # Violation, keep only first 2 rows
            result_df = pd.concat([result_df, group.iloc[:2]], ignore_index=True)
            print(f"  - Kept 2/{len(group)} rows for label {label}")
    
    print(f"After enforcing constraint: {len(result_df)} rows (removed {len(df) - len(result_df)} rows)")
    return result_df

def rebalance_combined_data(df, threshold, min_threshold):
    """
    Rebalance the combined data into appropriately sized clusters.
    
    Args:
        df: Combined DataFrame
        threshold: Maximum cluster size (pairs)
        min_threshold: Minimum cluster size (pairs)
        
    Returns:
        List of DataFrames representing rebalanced clusters
    """
    total_pairs = len(df) // 2
    print(f"Rebalancing {total_pairs} total pairs")
    
    # If total size is below threshold, return as single cluster
    if total_pairs <= threshold:
        return [df]
    
    # Calculate how many clusters we need
    num_clusters = (total_pairs + threshold - 1) // threshold  # Ceiling division
    print(f"Creating {num_clusters} balanced clusters")
    
    # If we have final_label column, use special splitting
    if 'final_label' in df.columns:
        return split_respecting_final_label(df, num_clusters)
    else:
        # Simple equal-sized splitting
        return split_into_equal_clusters(df, num_clusters)

def split_into_equal_clusters(df, num_clusters):
    """
    Split data into roughly equal-sized clusters.
    
    Args:
        df: DataFrame to split
        num_clusters: Number of clusters to create
        
    Returns:
        List of DataFrames
    """
    segment_size = len(df) // num_clusters
    clusters = []
    
    for i in range(num_clusters):
        start_idx = i * segment_size
        end_idx = (i + 1) * segment_size if i < num_clusters - 1 else len(df)
        cluster = df.iloc[start_idx:end_idx].copy()
        clusters.append(cluster)
        print(f"  - Cluster {i+1}: {len(cluster)//2} pairs")
    
    return clusters

def split_respecting_final_label(df, num_clusters):
    """
    Split data into clusters while respecting the final_label constraint.
    
    Args:
        df: DataFrame to split
        num_clusters: Target number of clusters
        
    Returns:
        List of DataFrames
    """
    # First, ensure no more than 2 rows per final_label in the entire dataset
    df = enforce_final_label_constraint(df)
    
    # Initialize empty clusters
    clusters = [pd.DataFrame(columns=df.columns) for _ in range(num_clusters)]
    
    # Group rows by final_label
    grouped = df.groupby('final_label')
    
    # Distribute rows with same final_label to same cluster when possible
    for label, group in grouped:
        # Always keep rows with same final_label together
        # Find the cluster with the most available space
        cluster_sizes = [len(cluster) for cluster in clusters]
        target_cluster = cluster_sizes.index(min(cluster_sizes))
        
        # Add all rows with this label to the target cluster
        clusters[target_cluster] = pd.concat([clusters[target_cluster], group], ignore_index=True)
    
    # Balance cluster sizes if needed
    min_size = min(len(cluster) for cluster in clusters)
    max_size = max(len(cluster) for cluster in clusters)
    
    # If clusters are too unbalanced, try to redistribute
    if max_size - min_size > 20:  # Allow some imbalance
        print(f"  - Balancing clusters (size range: {min_size//2}-{max_size//2} pairs)")
        
        # Sort clusters by size (largest first)
        clusters.sort(key=len, reverse=True)
        
        # Try to move rows from larger to smaller clusters
        for i in range(len(clusters) - 1):
            if len(clusters[i]) - len(clusters[-1]) <= 10:
                break  # Close enough
            
            # Find labels that appear only in the larger cluster
            if 'final_label' in clusters[i].columns:
                labels_to_move = []
                
                # Get unique labels in this cluster
                unique_labels = clusters[i]['final_label'].unique()
                
                for label in unique_labels:
                    label_rows = clusters[i][clusters[i]['final_label'] == label]
                    
                    # If this label doesn't appear in the smallest cluster, we can move it
                    if len(clusters[-1]) == 0 or label not in clusters[-1]['final_label'].values:
                        labels_to_move.append(label)
                
                # Move rows for one label
                if labels_to_move:
                    label_to_move = labels_to_move[0]
                    rows_to_move = clusters[i][clusters[i]['final_label'] == label_to_move]
                    
                    print(f"    - Moving {len(rows_to_move)} rows with label {label_to_move}")
                    
                    # Remove rows from source cluster
                    clusters[i] = clusters[i][clusters[i]['final_label'] != label_to_move]
                    
                    # Add rows to target cluster
                    clusters[-1] = pd.concat([clusters[-1], rows_to_move], ignore_index=True)
    
    # Final resorting by size
    clusters.sort(key=len, reverse=True)
    
    # Report final sizes
    for i, cluster in enumerate(clusters):
        print(f"  - Cluster {i+1}: {len(cluster)//2} pairs")
    
    return clusters

def save_cluster(df, output_folder, batch_num):
    """
    Save a cluster to a JSON file.
    
    Args:
        df: DataFrame to save
        output_folder: Where to save the file
        batch_num: Batch number for file naming
    """
    output_file = os.path.join(output_folder, f"Batch_rebalanced_{batch_num}.json")
    df.to_json(path_or_buf=output_file, orient='index', double_precision=10, indent=4, date_format='iso')
    
    pair_count = len(df) // 2
    
    if 'final_label' in df.columns:
        final_label_counts = df.groupby('final_label').size()
        max_count = final_label_counts.max() if not final_label_counts.empty else 0
        violates_constraint = (final_label_counts > 2).any()
        constraint_status = f"- VIOLATES final_label constraint (max count: {max_count})" if violates_constraint else ""
        print(f"Saved Batch_rebalanced_{batch_num}.json: {pair_count} pairs {constraint_status}")
    else:
        print(f"Saved Batch_rebalanced_{batch_num}.json: {pair_count} pairs")

if __name__ == "__main__":
    input_folder = "./input_json_files"
    output_folder = "./output_clusters"
    threshold = 70
    min_threshold = 30
    
    print(f"Starting cluster rebalancing with threshold: {threshold} pairs")
    print(f"Input folder: {input_folder}")
    print(f"Output folder: {output_folder}")
    
    process_input_folder(input_folder, output_folder, threshold, min_threshold)
    
    print("Cluster rebalancing completed")
